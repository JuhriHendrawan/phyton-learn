{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUGAS-31.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xBW7BBtKzY_"
      },
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oUsNdVVEUAT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reHd4RYEPgL5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cohx2KpCEiON"
      },
      "source": [
        "data = pd.read_json(\"gdrive/My Drive/public/data_latih.json\")#.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxkIDi71Eb6D"
      },
      "source": [
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWufQg5UIz3S"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umH2GeH4FiNN"
      },
      "source": [
        "### Membuat dictionary untuk membuat kata menjadi baku"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGvHEG4fFWEc"
      },
      "source": [
        "key = pd.read_csv(\"gdrive/My Drive/public/key_norm.csv\")\n",
        "kamus = key.set_index('singkat')['hasil'].to_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr16CxCzHAu2"
      },
      "source": [
        "### Fungsi untuk membersihkan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eL3QxVKZ55z"
      },
      "source": [
        "!pip install Sastrawi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM5DIkT6FA3d"
      },
      "source": [
        "# Clean character\n",
        "pat1 = r'@[A-Za-z0-9_]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "pat3 = r'RT'\n",
        "pat4 = r'#[A-Za-z0-9]+'\n",
        "combined_pat = r'|'.join((pat1,pat2,pat3,pat4))\n",
        "# Tokenization\n",
        "from nltk import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()\n",
        "# Stop word\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "#Stemmer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn3nUjnmGY5g"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "def tweet_cleaner(text):\n",
        "  soup = BeautifulSoup(text,'lxml')\n",
        "  souped = soup.get_text()\n",
        "  stripped = re.sub(combined_pat,' ',str(souped))\n",
        "\n",
        "  try:\n",
        "    clean = stripped.decode('utf-8').replace(u'\\ufffd','?')\n",
        "  except:\n",
        "    clean = stripped\n",
        "\n",
        "  letters_only = re.sub('[^a-zA-Z]',' ',clean)\n",
        "  lower_case = letters_only.lower()\n",
        "  words = tok.tokenize(lower_case)\n",
        "  change_words=[]\n",
        "  for w in words:\n",
        "    if w not in kamus:\n",
        "      change_words.append(w)\n",
        "    else:\n",
        "      change_words.append(kamus[w])\n",
        "  stemw = \" \".join(change_words).strip()\n",
        "#   stemmer = factory.create_stemmer()\n",
        "  stemmer1 = stemmer.stem(stemw)\n",
        "  wordsnew = tok.tokenize(stemmer1)\n",
        "  filtered_words = [w for w in wordsnew if not w in stop_words]\n",
        "  return (' '.join(filtered_words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqxDuY_vVLfF"
      },
      "source": [
        "%%time\n",
        "\n",
        "# print(\"Cleaning and parsing the tweets...\\n\")\n",
        "clean_text =[]\n",
        "for i in range(len(data)):\n",
        "  clean_text.append(tweet_cleaner(data['isi'][i]))\n",
        "print (\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7yZDt6OVqhh"
      },
      "source": [
        "clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPUiYLEGWZSf"
      },
      "source": [
        "data_tweet = pd.DataFrame({\n",
        "    'isi': clean_text,\n",
        "    'sentimen' : data['sentimen']})\n",
        "data_tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pIaykAnXsbd"
      },
      "source": [
        "tweet_pos = data_tweet[data_tweet.sentimen=='positif']\n",
        "tweet_nega = data_tweet[data_tweet.sentimen=='negatif']\n",
        "tweet_netr = data_tweet[data_tweet.sentimen=='netral']\n",
        "\n",
        "#menggabungkan masing-masing sentimen\n",
        "tweet_positif = \" \".join(tweet_pos['isi'])\n",
        "tweet_negatif = \" \".join(tweet_nega['isi'])\n",
        "tweet_netral = \" \".join(tweet_netr['isi'])\n",
        "print(tweet_positif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvcqqvyNjtfp"
      },
      "source": [
        "kata_pos = tok.tokenize(tweet_positif)\n",
        "kata_neg = tok.tokenize(tweet_negatif)\n",
        "kata_net = tok.tokenize(tweet_netral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcQaaOKaYEjA"
      },
      "source": [
        "print(\"Jumlah tweet positif: \", '{:4.2f}'.format(len(tweet_pos)/len(data)*100), '%', 'dengan kata dasar sebanyak: ', len(kata_pos))\n",
        "print(\"Jumlah tweet negatif: \", '{:4.2f}'.format(len(tweet_nega)/len(data)*100), '%', 'dengan kata dasar sebanyak: ', len(kata_neg))\n",
        "print(\"Jumlah tweet netral: \", '{:4.2f}'.format(len(tweet_netr)/len(data)*100), '%', 'dengan kata dasar sebanyak: ', len(kata_net))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w2QK-xCqd72"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsugWdqhYHMH"
      },
      "source": [
        "mask_pos = np.array(Image.open(\"gdrive/My Drive/public/conan1.png\"))\n",
        "mask_neg = np.array(Image.open(\"gdrive/My Drive/public/kid.png\"))\n",
        "mask_net = np.array(Image.open(\"gdrive/My Drive/public/twitter_logo.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkGX16MNplY1"
      },
      "source": [
        "wordcloud_pos = WordCloud(width=800,height=600,background_color=\"white\",mask=mask_pos).generate(tweet_positif)\n",
        "pyplot.clf()\n",
        "pyplot.imshow(wordcloud_pos)\n",
        "pyplot.axis('off')\n",
        "pyplot.title('Positif')\n",
        "#pyplot.savefig(\"indonesia-raya.png\",dpi = (300))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnrvQB14p32i"
      },
      "source": [
        "wordcloud_net = WordCloud(width=800,height=600,background_color=\"white\",mask=mask_net).generate(tweet_positif)\n",
        "pyplot.clf()\n",
        "pyplot.imshow(wordcloud_net)\n",
        "pyplot.axis('off')\n",
        "pyplot.title('Netral')\n",
        "#pyplot.savefig(\"indonesia-raya.png\",dpi = (300))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJLTFFmarH-Y"
      },
      "source": [
        "wordcloud_neg = WordCloud(width=800,height=600,background_color=\"white\",collocations=False).generate(tweet_positif)\n",
        "pyplot.clf()\n",
        "pyplot.imshow(wordcloud_neg)\n",
        "pyplot.axis('off')\n",
        "pyplot.title('Negatif')\n",
        "#pyplot.savefig(\"indonesia-raya.png\",dpi = (300))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNmdRAOnEmdY"
      },
      "source": [
        "### Membagi data dengan 20 % sebagai data test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtpqpytazEN1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "np.random.seed(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWX_PyJOoFvY"
      },
      "source": [
        "class_name=set(data_tweet['sentimen'])\n",
        "class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UoVSUsEFmJp"
      },
      "source": [
        "data_tweet.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80AmLDhjxX0W"
      },
      "source": [
        "trainSet, testSet = train_test_split(data_tweet, test_size = 0.2, stratify = data_tweet['sentimen'], random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlqqGRHGy7bW"
      },
      "source": [
        "count_vect = CountVectorizer(analyzer=\"word\", ngram_range=(1,1), stop_words = stopwords.words('english'), lowercase=True)\n",
        "\n",
        "train_vect = count_vect.fit_transform(trainSet['isi'].astype(str)) #fit transform pada data train A\n",
        "test_vect = count_vect.transform(testSet['isi'].astype(str)) #transform data test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjYhR2c3zISi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmCd6VWm17ia"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKjM755N1tki"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lf_WUrzlJK-"
      },
      "source": [
        "train_vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAKtGFvv2Bt2"
      },
      "source": [
        "AdaClassifier = AdaBoostClassifier()\n",
        "# AdaClassifier = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=10.0, n_estimators=100, random_state=42)\n",
        "AdaClassifier.fit(train_vect , trainSet['sentimen'])\n",
        "Predict1 = AdaClassifier.predict(test_vect)\n",
        "accuracy_1 = accuracy_score(Predict1, testSet['sentimen'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6DoIQ2p2Mjr"
      },
      "source": [
        "print (\"Accuracy pada test data dengan AdaBoost:\", accuracy_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2tFmHJj3Gmr"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "\n",
        "SVM.fit(train_vect , trainSet['sentimen'])\n",
        "Predict2 = SVM.predict(test_vect)\n",
        "accuracy_2=accuracy_score(Predict2, testSet['sentimen'])\n",
        "print (\"Accuracy pada test data dengan SVM:\", accuracy_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKxTYXY8q9KF"
      },
      "source": [
        "sentimen = {'negatif': '0','positif': '1','netral': '2'} #ubah sentiment menjadi angka\n",
        "trainSet.sentimen = [sentimen[item] for item in trainSet.sentimen]\n",
        "testSet.sentimen = [sentimen[item] for item in testSet.sentimen]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRGmxNv7r9bp"
      },
      "source": [
        "trainSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXEe-wglrwH6"
      },
      "source": [
        "testSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHpldlYpQQJ"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(train_vect , trainSet['sentimen'])\n",
        "Predict3 = model.predict(test_vect)\n",
        "r_sq = model.score(train_vect , trainSet['sentimen'])\n",
        "print('Coefficient of determination (R^2):', r_sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZM3BlSGs6IB"
      },
      "source": [
        "print (\"Accuracy pada test data dengan AdaBoost:\", accuracy_1)\n",
        "print (\"Accuracy pada test data dengan SVM:\", accuracy_2)\n",
        "print('Coefficient Regresi Linier (R^2):', r_sq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU66mpfxF3gv"
      },
      "source": [
        "### Confussion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP_J8eU6vtMZ"
      },
      "source": [
        "sentimen_balik = {'0' : 'negatif','1' : 'positif', '2': 'netral'} #ubah sentiment menjadi angka\n",
        "trainSet.sentimen = [sentimen_balik[item] for item in trainSet.sentimen]\n",
        "testSet.sentimen = [sentimen_balik[item] for item in testSet.sentimen]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSa0k-ZMCYI0"
      },
      "source": [
        "class_name=set(data_tweet['sentimen']) #mendefinisikan class dari data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQXGVos9Chlq"
      },
      "source": [
        "class_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBYXGYqr56SU"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01rOAN1dwbAO"
      },
      "source": [
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(testSet[\"sentimen\"] , Predict1, classes=class_name,\n",
        "                      title='Confusion matrix, AdaBoost')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EStqLGKjws7e"
      },
      "source": [
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(testSet[\"sentimen\"] , Predict2, classes=class_name,\n",
        "                      title='Confusion matrix, SVM')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny8LXdJiw2Qk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}